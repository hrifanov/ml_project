{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 17:52:05.796446: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-12-10 17:52:05.797467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-12-10 17:52:05.797476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_path = Path('../../../Dataset/img_celeba_cropped/')\n",
    "celeba_identity_path = Path('../../Dataset/celeb_identity_processed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_identity = pd.read_csv(\n",
    "    celeba_identity_path,\n",
    "    sep=\",\",\n",
    "    names=[\"image\", \"identity\"],\n",
    "    header=0,\n",
    "    dtype=str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_count = 5000\n",
    "train_size = 0.7\n",
    "network_input_shape = (224, 224, 3)\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_triplets(df_identity, n):\n",
    "    triplets = []\n",
    "\n",
    "    for triplet in range(n):\n",
    "        anchor_id = df_identity['identity'].sample(1, replace=True).to_string(index=False)\n",
    "\n",
    "        keepGoing = True\n",
    "        while (keepGoing):\n",
    "            anchor, positive = df_identity.loc[df_identity['identity'] == anchor_id]['image'].sample(2, replace=True)\n",
    "            keepGoing = (anchor == positive)\n",
    "\n",
    "        negative = df_identity.loc[df_identity['identity'] != anchor_id]['image'].sample(1, replace=True).to_string(index=False)\n",
    "\n",
    "        anchor = str(celeba_path) + '/' + anchor\n",
    "        positive = str(celeba_path) + '/' + positive\n",
    "        negative = str(celeba_path) + '/' + negative\n",
    "\n",
    "        triplets.append([anchor, positive, negative])\n",
    "\n",
    "    return pd.DataFrame(triplets, columns=['anchor', 'positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = create_n_triplets(df_identity, triplets_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(anchor, positive, negative):\n",
    "    return (\n",
    "        convert_to_img(anchor),\n",
    "        convert_to_img(positive),\n",
    "        convert_to_img(negative)\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_img(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = resnet50.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.data.Dataset.from_tensor_slices((\n",
    "    triplets['anchor'].values,\n",
    "    triplets['positive'].values,\n",
    "    triplets['negative'].values\n",
    "))\n",
    "\n",
    "features = features.map(preprocessing)\n",
    "fake_labels = tf.data.Dataset.from_tensor_slices(np.zeros(triplets_count))\n",
    "dataset = tf.data.Dataset.zip((features, fake_labels))\n",
    "\n",
    "train_dataset = dataset.take(round(triplets_count * train_size))\n",
    "val_dataset = dataset.skip(round(triplets_count * train_size))\n",
    "\n",
    "train_dataset = train_dataset \\\n",
    "        .batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE) \\\n",
    "        .cache()\n",
    "\n",
    "val_dataset = val_dataset \\\n",
    "        .batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_embedding(trainable_layers, n_hidden, n_neurons, dropout_rate):\n",
    "    input = layers.Input(shape=network_input_shape)\n",
    "\n",
    "    base_cnn = resnet50.ResNet50(\n",
    "        input_tensor=input,\n",
    "        input_shape=network_input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    for layer in base_cnn.layers[0:-trainable_layers]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = layers.Flatten()(base_cnn.output)\n",
    "\n",
    "    for _ in range(n_hidden):\n",
    "        model = tf.keras.layers.Dropout(dropout_rate)(model)\n",
    "        model = tf.keras.layers.Dense(n_neurons, activation='relu')(model)\n",
    "\n",
    "    model = layers.Dense(512)(model)\n",
    "\n",
    "    return Model(input, outputs=model, name='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(layers.Layer):\n",
    "\n",
    "    def __init__(self, margin, **kwargs):\n",
    "        self.margin = margin\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def euclidean_distance(self, x, y):\n",
    "        sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
    "        return tf.sqrt(tf.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        positive_distance = self.euclidean_distance(anchor, positive)\n",
    "        negative_distance = self.euclidean_distance(anchor, negative)\n",
    "\n",
    "        loss = positive_distance - negative_distance\n",
    "\n",
    "        return tf.maximum(loss + self.margin, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    trainable_layers = hp.Int('trainable_layers', min_value=1, max_value=3, step=1, default=1)\n",
    "    n_hidden = hp.Int('n_hidden', min_value=0, max_value=5, default=1)\n",
    "    n_neurons = hp.Int('n_neurons', min_value=32, max_value=512, step=32)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0, max_value=0.8, step=0.1)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-6, max_value=1e-2, sampling='log')\n",
    "\n",
    "    anchor_input = layers.Input(shape=network_input_shape, name='anchor_input')\n",
    "    positive_input = layers.Input(shape=network_input_shape, name='right_input')\n",
    "    negative_input = layers.Input(shape=network_input_shape, name='negative_input')\n",
    "\n",
    "    embedding = resnet_embedding(trainable_layers, n_hidden, n_neurons, dropout_rate)\n",
    "\n",
    "    distances = TripletLoss(margin=1)(\n",
    "        embedding(anchor_input),\n",
    "        embedding(positive_input),\n",
    "        embedding(negative_input)\n",
    "    )\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[anchor_input, positive_input, negative_input],\n",
    "        outputs=distances,\n",
    "        name='resnet50',\n",
    "    )\n",
    "\n",
    "    model.compile(loss=identity_loss,\n",
    "                  optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_opt_tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    seed=42,\n",
    "    max_trials=20,\n",
    "    alpha=1e-3,  # to learn (1e-4 default)\n",
    "    beta=2.6,  # to learn\n",
    "    overwrite=True,\n",
    "    directory=\"celeba_tuning\",\n",
    "    project_name=\"bayesian_opt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    restore_best_weights=False,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "lr_scheduler_reduce = keras.callbacks.ReduceLROnPlateau(\n",
    "    factor=0.1,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(os.path.join(bayesian_opt_tuner.project_dir, 'tensorboard'))\n",
    "\n",
    "callbacks_list = [\n",
    "    early_stopping,\n",
    "    lr_scheduler_reduce,\n",
    "    tensorboard,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 08m 27s]\n",
      "val_loss: 0.9446244239807129\n",
      "\n",
      "Best val_loss So Far: 0.9115082025527954\n",
      "Total elapsed time: 02h 26m 14s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "bayesian_opt_tuner.search(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for celeba_tuning/bayesian_opt/trial_05/checkpoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79/604220232.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_three_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayesian_opt_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \"\"\"\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Method only exists in this class for the docstring override.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \"\"\"\n\u001b[1;32m    295\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \"\"\"\n\u001b[1;32m    295\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Only load weights to avoid loading `custom_objects`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_distribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_checkpoint_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for celeba_tuning/bayesian_opt/trial_05/checkpoint"
     ]
    }
   ],
   "source": [
    "top_three_models = bayesian_opt_tuner.get_best_models(num_models=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_three_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79/4207174105.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_three_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'top_three_models' is not defined"
     ]
    }
   ],
   "source": [
    "top_three_models[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_three_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79/893914118.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_three_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'top_three_models' is not defined"
     ]
    }
   ],
   "source": [
    "top_three_models[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_three_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79/2609496826.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_three_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'top_three_models' is not defined"
     ]
    }
   ],
   "source": [
    "top_three_models[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

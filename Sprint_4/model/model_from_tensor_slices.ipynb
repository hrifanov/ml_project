{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import resnet50, ResNet50\n",
    "from tensorflow.keras.utils import plot_model, image_dataset_from_directory\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import convert_to_tensor\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '../../../Dataset/img_celeba_cropped'\n",
    "identity_path = '../../../Dataset/identity_CelebA.txt'\n",
    "attributes_path = '../../../Dataset/list_attr_celeba.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_num = 50000\n",
    "image_shape = (224, 224, 3)\n",
    "\n",
    "batch_size = 32\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_identity = pd.read_csv(identity_path, sep = \" \", names=[\"image\", \"identity\"])[:images_num-1]\n",
    "celeb_identity[\"identity\"] = celeb_identity[\"identity\"].astype(\"string\")\n",
    "\n",
    "celeb_attrs = pd.read_csv(attributes_path, sep = \"\\s+\")[:images_num-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_pair(column:str, pairing_column:str, df=celeb_identity):\n",
    "    # getting random identity from the provided column to use for balanced pair\n",
    "    random_id = df[column].sample(1, replace=True).to_string(index=False)\n",
    "    containing_id_list = df.loc[df[column] == random_id][pairing_column].sample(2, replace=True).to_list()\n",
    "    \n",
    "    # random pictures for pair generation\n",
    "    df_without_id = df.loc[df[pairing_column] != random_id]\n",
    "    negative = df[pairing_column].sample(1, replace=True).to_list()\n",
    "    \n",
    "    return [[containing_id_list[0], containing_id_list[1], 1], [containing_id_list[0], negative[0], 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_pairs = []\n",
    "\n",
    "for record in range(10000):\n",
    "    pair = get_balanced_pair(\"identity\", \"image\")\n",
    "    balanced_pairs.append(pair[0])\n",
    "    balanced_pairs.append(pair[1])\n",
    "    \n",
    "df = pd.DataFrame(balanced_pairs, columns =['left', 'right', 'label'])\n",
    "df['left'] = images_path + '/' + df['left']\n",
    "df['right'] = images_path + '/' + df['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/002084.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/034289.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/002084.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/022060.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/035540.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/021204.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/035540.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/042820.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/002545.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/013740.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             left  \\\n",
       "0  ../../../Dataset/img_celeba_cropped/002084.jpg   \n",
       "1  ../../../Dataset/img_celeba_cropped/002084.jpg   \n",
       "2  ../../../Dataset/img_celeba_cropped/035540.jpg   \n",
       "3  ../../../Dataset/img_celeba_cropped/035540.jpg   \n",
       "4  ../../../Dataset/img_celeba_cropped/002545.jpg   \n",
       "\n",
       "                                            right  label  \n",
       "0  ../../../Dataset/img_celeba_cropped/034289.jpg      1  \n",
       "1  ../../../Dataset/img_celeba_cropped/022060.jpg      0  \n",
       "2  ../../../Dataset/img_celeba_cropped/021204.jpg      1  \n",
       "3  ../../../Dataset/img_celeba_cropped/042820.jpg      0  \n",
       "4  ../../../Dataset/img_celeba_cropped/013740.jpg      1  "
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pairs into TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((df['left'].values, df['right'].values, df['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(left, right, label):\n",
    "    left_image = convert_to_img(left)\n",
    "    right_image = convert_to_img(right)\n",
    "    \n",
    "    return left_image, right_image, label\n",
    "\n",
    "\n",
    "def convert_to_img(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = resnet50.preprocess_input(img)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.reshape(img, (224,224))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocessing)\n",
    "\n",
    "dataset.batch(batch_size, num_parallel_calls=AUTOTUNE)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec=(TensorSpec(shape=(224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(round(images_num * 0.8))\n",
    "val_dataset = dataset.skip(round(images_num * 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = layers.Input(shape=image_shape, name='left_input', dtype=tf.float32)\n",
    "right_input = layers.Input(shape=image_shape, name='right_input', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet=ResNet50(\n",
    "    include_top=False,\n",
    "    input_shape=input_shape,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    classes=2)\n",
    "\n",
    "for layer in resnet.layers[0:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "left_resnet = resnet(left_input)\n",
    "right_resnet = resnet(right_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging resnet branches layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.reduce_sum(tf.square(subtract(x, y)), axis=1, keepdims=True)\n",
    "    return tf.sqrt(tf.maximum(sum_square, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = layers.subtract([left_resnet, right_resnet])\n",
    "#merged = layers.Lambda(euclidean_distance, name=\"merge\")([left_resnet, right_resnet])\n",
    "\n",
    "flatten = layers.Flatten()(merged)\n",
    "\n",
    "merged_dense_1 = layers.Dense(256, activation='relu')(flatten)\n",
    "merged_dense_2 = layers.Dense(32, activation='relu')(merged_dense_1)\n",
    "\n",
    "merged_output = layers.Dense(1, activation='sigmoid')(merged_dense_2)\n",
    "\n",
    "model = Model(inputs = [left_input, right_input], outputs = merged_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "# margin is a parametr settable by developer\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        '''Contrastive loss from Hadsell-et-al.'06\n",
    "        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "        '''\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "        return (y_true * square_pred + (1 - y_true) * margin_square)    \n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=lr), loss=contrastive_loss_with_margin(margin=1.0), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " left_input (InputLayer)        [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " right_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " resnet50 (Functional)          (None, 2048)         23587712    ['left_input[0][0]',             \n",
      "                                                                  'right_input[0][0]']            \n",
      "                                                                                                  \n",
      " subtract_4 (Subtract)          (None, 2048)         0           ['resnet50[0][0]',               \n",
      "                                                                  'resnet50[1][0]']               \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 2048)         0           ['subtract_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 256)          524544      ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 32)           8224        ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1)            33          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,120,513\n",
      "Trainable params: 532,801\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#plot_model(resnet, show_shapes=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_4\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(224, 224) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [696], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/mx/1v4nf8c52yv1vdfw99lyg6qh0000gn/T/__autograph_generated_file_0eayrxs.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_4\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(224, 224) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(inpust = train_dataset, epochs=5, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(combo_model_fitted.history['accuracy'])\n",
    "plt.plot(combo_model_fitted.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.axis(ymin=0.5,ymax=1)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(combo_model_fitted.history['loss'])\n",
    "plt.plot(combo_model_fitted.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.axis(ymin=0,ymax=0.5)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_feature_vectors = model.predict(custom_img_left)\n",
    "left_image_feature_vectors = model.predict(custom_img_right)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

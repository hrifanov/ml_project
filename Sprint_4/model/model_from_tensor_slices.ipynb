{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.utils import plot_model, image_dataset_from_directory\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import convert_to_tensor\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '../../../Dataset/img_celeba_cropped'\n",
    "identity_path = '../../../Dataset/identity_CelebA.txt'\n",
    "attributes_path = '../../../Dataset/list_attr_celeba.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_num = 50000\n",
    "image_shape = (224, 224, 3)\n",
    "\n",
    "batch_size = 32\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_identity = pd.read_csv(identity_path, sep = \" \", names=[\"image\", \"identity\"])[:images_num-1]\n",
    "celeb_identity[\"identity\"] = celeb_identity[\"identity\"].astype(\"string\")\n",
    "\n",
    "celeb_attrs = pd.read_csv(attributes_path, sep = \"\\s+\")[:images_num-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_pair(column:str, pairing_column:str, df=celeb_identity):\n",
    "    # getting random identity from the provided column to use for balanced pair\n",
    "    random_id = df[column].sample(1, replace=True).to_string(index=False)\n",
    "    containing_id_list = df.loc[df[column] == random_id][pairing_column].sample(2, replace=True).to_list()\n",
    "    \n",
    "    # random pictures for pair generation\n",
    "    df_without_id = df.loc[df[pairing_column] != random_id]\n",
    "    negative = df[pairing_column].sample(1, replace=True).to_list()\n",
    "    \n",
    "    return [[containing_id_list[0], containing_id_list[1], 1], [containing_id_list[0], negative[0], 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_pairs = []\n",
    "\n",
    "for record in range(100):\n",
    "    pair = get_balanced_pair(\"identity\", \"image\")\n",
    "    balanced_pairs.append(pair[0])\n",
    "    balanced_pairs.append(pair[1])\n",
    "    \n",
    "df = pd.DataFrame(balanced_pairs, columns =['left', 'right', 'label'])\n",
    "df['left'] = images_path + '/' + df['left']\n",
    "df['right'] = images_path + '/' + df['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/021509.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/042762.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/021509.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/027436.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/044078.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/046414.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/044078.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/025159.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../../Dataset/img_celeba_cropped/018977.jpg</td>\n",
       "      <td>../../../Dataset/img_celeba_cropped/037049.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             left  \\\n",
       "0  ../../../Dataset/img_celeba_cropped/021509.jpg   \n",
       "1  ../../../Dataset/img_celeba_cropped/021509.jpg   \n",
       "2  ../../../Dataset/img_celeba_cropped/044078.jpg   \n",
       "3  ../../../Dataset/img_celeba_cropped/044078.jpg   \n",
       "4  ../../../Dataset/img_celeba_cropped/018977.jpg   \n",
       "\n",
       "                                            right  label  \n",
       "0  ../../../Dataset/img_celeba_cropped/042762.jpg      1  \n",
       "1  ../../../Dataset/img_celeba_cropped/027436.jpg      0  \n",
       "2  ../../../Dataset/img_celeba_cropped/046414.jpg      1  \n",
       "3  ../../../Dataset/img_celeba_cropped/025159.jpg      0  \n",
       "4  ../../../Dataset/img_celeba_cropped/037049.jpg      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pairs into TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = tf.data.Dataset.from_tensor_slices((df['left'].values, df['right'].values, df['label'].values))\n",
    "\n",
    "dataset_features = tf.data.Dataset.from_tensor_slices((df['left'].values, df['right'].values))\n",
    "\n",
    "train_label_dataset = tf.data.Dataset.from_tensor_slices(df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(left, right):\n",
    "        \n",
    "    return convert_to_img(left), convert_to_img(right)\n",
    "\n",
    "\n",
    "def convert_to_img(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    #img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def label_preprocessing(label):\n",
    "    return tf.cast(label, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = dataset_features.map(preprocessing)\n",
    "train_label_dataset = train_label_dataset.map(label_preprocessing)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.zip((dataset_features, train_label_dataset))\n",
    "\n",
    "dataset.batch(batch_size, num_parallel_calls=AUTOTUNE)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(round(images_num * 0.8))\n",
    "val_dataset = dataset.skip(round(images_num * 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = layers.Input(shape=(224, 224, 3), name='left_input')\n",
    "right_input = layers.Input(shape=(224, 224, 3), name='right_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n",
      "19 GlobalAveragePooling2D True\n"
     ]
    }
   ],
   "source": [
    "vgg=vgg16.VGG16(\n",
    "    include_top=False,\n",
    "    input_shape=image_shape,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    classes=2)\n",
    "\n",
    "for layer in vgg.layers[0:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for (i,layer) in enumerate(vgg.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)\n",
    "    \n",
    "left_vgg16 = vgg(left_input)\n",
    "right_vgg16 = vgg(right_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging resnet branches layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.reduce_sum(tf.square(subtract(x, y)), axis=1, keepdims=True)\n",
    "    return tf.sqrt(tf.maximum(sum_square, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = layers.subtract([left_vgg16, right_vgg16])\n",
    "#merged = layers.Lambda(euclidean_distance, name=\"merge\")([left_resnet, right_resnet])\n",
    "\n",
    "flatten = layers.Flatten()(merged)\n",
    "\n",
    "merged_dense_1 = layers.Dense(256, activation='relu')(flatten)\n",
    "merged_dense_2 = layers.Dense(32, activation='relu')(merged_dense_1)\n",
    "\n",
    "merged_output = layers.Dense(1, activation='sigmoid')(merged_dense_2)\n",
    "\n",
    "model = Model(inputs = [left_input, right_input], outputs = merged_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# margin is a parametr settable by developer\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        '''Contrastive loss from Hadsell-et-al.'06\n",
    "        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "        '''\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "        return (y_true * square_pred + (1 - y_true) * margin_square)    \n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=lr), loss=contrastive_loss_with_margin(margin=1.0), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " left_input (InputLayer)        [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " right_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " vgg16 (Functional)             (None, 512)          14714688    ['left_input[0][0]',             \n",
      "                                                                  'right_input[0][0]']            \n",
      "                                                                                                  \n",
      " subtract_1 (Subtract)          (None, 512)          0           ['vgg16[0][0]',                  \n",
      "                                                                  'vgg16[1][0]']                  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 512)          0           ['subtract_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          131328      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           8224        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            33          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,854,273\n",
      "Trainable params: 139,585\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#plot_model(resnet, show_shapes=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 279, in __call__\n        batch_dim = tf.shape(y_t)[0]\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/mx/1v4nf8c52yv1vdfw99lyg6qh0000gn/T/__autograph_generated_filenwg4a69f.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/igor/miniforge3/envs/my_env/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 279, in __call__\n        batch_dim = tf.shape(y_t)[0]\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(train_dataset, epochs=5, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(combo_model_fitted.history['accuracy'])\n",
    "plt.plot(combo_model_fitted.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.axis(ymin=0.5,ymax=1)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(combo_model_fitted.history['loss'])\n",
    "plt.plot(combo_model_fitted.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.axis(ymin=0,ymax=0.5)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_feature_vectors = model.predict(custom_img_left)\n",
    "left_image_feature_vectors = model.predict(custom_img_right)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
